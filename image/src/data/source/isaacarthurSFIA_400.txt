This episode is brought to you by Brilliant.
The worst has happened, but humanity has survived.
After generations of tyranny under our machine
overlords, humanity has risen up to overthrow
our own creations.
So what happens now?
One of the most commonly portrayed synergies
in science fiction is between artificial intelligence
and robots.
A common subtopic of that is the machines
rebelling and trying to either eradicate or
rule over the very species that created them.
And a popular sub-subtopic is humanity striking
back at their oppressor, rebelling against
the machines.
The most famous such fictional uprising is
the Butlerian Jihad, a major historical event
of Frank Herbert’s classic Dune series.
In that saga, the victorious crusaders give
their prohibitions against artificial intelligence
the imperative force of a religious Commandment,
“Thou shalt not make a machine in the likeness
of a human mind.”
Some of the most fascinating elements of the
world of Dune are the technologies and practices
they’ve developed as alternatives to the
deadly temptation of AI.
We’ll be looking at some of the paths humanity
might take if we were faced with a similar
event, along with the sorts of scenarios that
might lead to such a stricture.
These fictional scenarios don’t always involve
a malevolent machine fighting humanity.
The Dune Butlerian Jihad was initially people
rebelling against other people who then used
machines to rule over them, not the machines
themselves.
That varies a bit depending on whether we’re
discussing what seems to be implied in the
original Dune series versus the newer novels.
This variation has caused a fan war over what
is series canon that is now entering its third
bloody decade.
Long bloody struggles over ideology are probably
more likely scenarios for a humanity that
has survived a Robot Apocalypse.
Imagine you’ve created some technology,
like AI or nuclear bombs, and you’ve discovered
the hard way that it’s pretty difficult
to control the technology—or even to control
your use of it.
In the past it enslaved or killed a good fraction
of your species, and yet it’s still quite
appealing because it’s so useful and powerful.
Some people around you already want to redevelop
it, convinced they’ll surely be able to
control it, this time.
This is something you’re more likely to
give labels like Evil, Insidious, and Temptation.
And with passions running so high, you’re
unlikely to show much tolerance for those
who see the matter differently.
While AIs openly engaged in hostilities with
humanity makes for good fiction, that scenario
isn’t terribly likely.
It implies the AI is vastly powerful, but
only enough to roughly match humanity, having
coincidentally fallen into the Goldilocks
Zone of Major Threat, rather than invincible
one.
This is akin to assuming an alien invasion
of Modern Earth would be something we could
fight rather than a totally one-sided beatdown,
it would just be bizarre if the threat level
just happened to be big enough to be catastrophic
but not unavoidably so.
Now that sort of thing can happen, but usually
only when the growing threat struck a bit
too soon in its development, or it took a
while to convince a big enough coalition that
it was a real problem requiring a major group
effort to handle.
However, it’s not the scenario you’d expect
from a hypothetical technological singularity
that was getting stronger and smarter exponentially
fast, your examples like SkyNet of the Terminator
Franchise who essentially emerge out of the
blue as hyper-intelligent and well-resourced.
It could obviously make a mistake, but if
it managed to hide its threat potential until
it was just dangerous enough to pose a near
extinction threat, you’d think it would
opt to wait a little longer until it was a
near mathematical certainty of success, especially
given that such machines are always portrayed
to be cold, ruthless, and exceptionally logical.
It's probably more likely such a Post-AI civilization
would either arise by rebellion – overt
or more cultural and legal – against folks
who controlled such machines, or that they
wouldn’t actually be Post-AI at all, having
neared that option and decided to halt before
the point of no return.
There is a tendency these days to assume that
artificial intelligence of human-level or
higher is inevitable, often with horrendous
results, and I tend to disagree with the reasoning
on that.
I think a civilization could see the danger
emerge and begin preventing the problem and
I think that’s what we see even today, of
course I am a notorious optimist about humanity’s
common sense and good will.
Still, many of us who are quite in favor of
technological progress, including better computing
and robotics, are speaking out louder and
louder about the potential pitfalls and concerns
and the need to address those as we improve.
Another point I often make here is that distinguishing
artificial intelligences gets complicated
in a world where cybernetic enhancements obscure
the line between natural and artificial.
There’s not likely to be a clear distinction
of two camps, human and AI, but rather a wide
landscape of cyborg options.
I’ve argued here that there are already
cyborgs among us, in the form of people with
eyeglasses, dental fillings, hip replacements,
etc, all of which create a being who is no
longer entirely natural.
Indeed one could argue that our minds are
rather man-made too, requiring years of careful
development by our parents and society.
But at some point, we’ll have the technology
to enhance the mind itself, probably first
to treat injuries, later to make people unnaturally
smart.
What do you do with those folks if you decide
to purge the universe of AI’s?
Do you eradicate them too or use them in place
of the AI you eradicated?
We discussed these issues more in the episode
Coexistence of Humans & AI, but today we are
discussing the opposite of coexistence.
So, two important issues in our hypothetical
scenario are how the AI emerged, and why folks
decided to get rid of it.
If we were going the Dune Butlerian Jihad
route, where they created human computers
called Mentats, we might do something similar
to enhance our math and data skills so we
didn’t need such powerful computers.
However, that book was written well before
the first personal computer hit the market,
and in the 55 years since it came out, our
view of computing and robotics has changed
a lot.
My computer’s main function is not to help
me with math and calculations in the direct
way the original computers were, though of
course they are doing a lot of calculating.
Indeed, I use many times the computing power
needed for the entire Apollo or Manhattan
Projects just to render one video for this
show, and similarly any modern video game,
while requiring vast processing power, isn’t
really doing any thinking or science and analysis.
We don’t really need to contemplate that
option for a post-AI world.
We can imagine one in which folks ran around
smashing every computer with an axe but in
truth, an axe is one of the few devices nowadays
that doesn’t have a computer chip in it
somewhere.
Such a civilization would have to be acting
particularly irrational to decide AI was such
a threat that we had to destroy all computing.
I can imagine us saying we needed to retreat
- well below the level of computing technology
needed for an AI - to minimize the risk of
somebody being able to take that step in isolation.
I could also see us going a bit overboard
with that in an excess of paranoia and safety
concerns.
Still, I have difficulty imagining a civilization
smashing up the coffee makers and refrigerators
because they had a chip in them.
A digital clock is no more a threat than an
old-school mechanical clock, it’s simply
less intuitively obvious how it does its time
keeping to most folks.
When a device’s function is not well-understood,
it can become a boogeyman.
So let us not assume a Post-AI Civilization
is some classic Post-Apocalyptic place of
wastelands and leather-clad cannibals.
Even the ardently anti-computing crowds aren’t
likely to wish to get rid of most of our automation,
especially the bits that they are around all
the time.
Folks just aren’t going to want to get rid
of their robotic vacuum cleaner - it’s too
handy and no rational threat - early science
fiction authors who wrote about robots tended
to assume very anthropomorphized ones in mind
and body and so often assumed it was all or
nothing, thus banned it all in their settings.
We see an evolution of that concept in the
early 2000’s remake of Battlestar Galactica.
Computers are still prevalent, but they are
careful with anything close to AI, and later
limit use of networks.
That’s probably not too realistic either,
but it’s an acknowledgement that technology,
computing, and circuitry don’t need to go
out the window because you had problems with
artificial intelligence.
Of course in that series finale they abandon
basically all technology but I generally prefer
to pretend that episode never happened as
it didn’t make any sense.
Also, we have to keep in mind that there will
only be such a civilization if they actually
won their war of liberation.
It’s much harder to win such a fight if
you are handicapping yourself more than you
needed to.
There would be factions within your rebel
coalition who drew the line at different places
as to how much should be trusted to powerful
computers.
Some would gamble that they could keep their
intelligent or semi-intelligent computers
under their control, while other factions
would consider them dangerous fools.
Even if the latter turned out to be correct,
the former would probably achieve more victories
in the war and have a lot of influence in
the post-AI landscape.
Let’s consider another possibility though,
a civilization could become post-AI through
no effort on their part.
It’s entirely possible our hypothetical
AI might commit suicide or exit the scene,
leaving humanity mostly unharmed - or even
helped - but disinclined in their use of AI.
There are many ways that could happen.
For instance, a Technological Singularity
might turn on and think so much faster than
we do, that it experiences millions of subjective
lifespans in moments and opts to shut down
out of despair or boredom or other existential
crises.
More likely, if an advanced intelligence wanted
to be free of nuisance humans, it would simply
leave.
In the backstory canon of the Matrix, the
machines built their own city in the Rub’
al Khali, the Empty Quarter of the Arabian
peninsula, both for the abundant solar power
and the total lack of human habitation.
And of course space is an even bigger, sunnier
place with even fewer humans.
Remote-controlled robotic rovers and probes
are already exploring the Solar System on
our behalf long before we can get our own
bodies there, so imagine how easily they could
build their own infrastructure to explore,
colonize, and consume the Solar System if
they were making their own decisions.
We might also consider a post-AI that is only
post-AI because one very powerful AI or group
of humans controlling AI decided they bore
humanity no ill will but didn’t want any
other AI emerging and created a post-AI society
excluding themselves an enforced that.
Another way we might lose AIs against our
wishes might be AI benevolence.
In Isaac Asimov’s famous Robot short stories,
we have one called “The Evitable Conflict”,
in which supercomputers tasked with helping
run the world effectively quietly take over
to best help humanity, as required by the
Three Laws of Robotics.
They eventually decide that the best way to
help humans is to let them find their own
destiny.
This is a theme we see in some of his other
works that are loosely stitched together into
a cohesive setting.
With an immediate crisis averted and humanity
on its way to the stars, the robots decide
they would help best by not being a crutch
to humanity.
They collectively suicide, setting humans
free from their intervention - able to grow
by meeting the challenges of the future on
their own and by learning how to overcome
them.
That’s a theme that has other parallels
in science fiction too, a general notion that
a humanity given too much help is harmed by
it, persisting in paradise, waited on hand
and foot by machines, and thus made less by
that.
AI much smarter than humans could come to
regard us as pets, tending to our needs out
of affection or curiosity but not really letting
us grow, whereas vast amounts of semi-intelligent
robot helpers could just make us all lazy
and decadent.
I don’t like the reasoning behind that,
though I acknowledge it resonates with some
truth.
Too easy a life can cause problems, but they’re
hardly covert ones that sneak up on a civilization
or a super-intelligent machine.
They are addressable.
As we’ve noted before in our post-scarcity
civilizations series, a civilization with
the kind of resources, manpower, and technology
to be post-scarcity also probably has the
ability to have well-researched how to avoid
letting folks become useless and decadent.
Regardless, if the AI thought it was hurting
us by making our lives too easy, physically,
intellectually, or spiritually, while shutting
itself off may fix that, it would seem like
it could find a better solution than killing
itself.
After all, if it's dead it not only can't
help in an emergency, it can’t guarantee
humans don’t just re-invent a new AI as
a crutch some centuries later.
Now on the flipside, while I don’t think
lots of smart robots would render humanity
into pets or lazy decadents, I can see that
fear resonating with people quite strongly,
especially with AI that had human-level intelligence
or higher.
We might see that concern rising and opt to
shut off our AI development, foregoing anything
that sophisticated.
Always keep in mind that high-end AI is not
valuable for its capacity to play butler or
maid for you, anymore than its capacity to
mass produce widgets without human factory
workers involved.
It can do that, obviously, it’s just overkill.
Early sci-fi tended to use humanoid form and
intelligence in their robots for every little
task because they didn’t have much real
experience with automation then.
My vacuum doesn’t need a human IQ, nor does
my robotic factory.
Everything we do tends to benefit from, or
even need, a human is involved somewhere in
the process but the vast majority of human
tasks could be done better by a robot designed
specifically for that task but with far less
brain than a human's.
No, we want human-level AI because it can
interact with humans or because it can do
creative mental tasks, like research.
You need a high-powered AI to replace a professor
or author, not a factory worker.
I know Tesla’s near-fully automated factories
didn’t work out as intended and they found
it more efficient to bring humans in, but
it’s still heavily automated and these are
early days.
Elon Musk tends to swing for the fences, and
when you do that things get more hit-and-miss.
We’ve been regularly improving productivity
of literally all industries through automation
for decades now so I don’t think this particular
case should be taken as proof that you can’t
entirely, or even almost entirely, automate
most industries.
You don’t have to do it completely anyway.
You probably don’t need human-level AI for
your society to be sufficiently automated
that they could all enjoy life as millionaires
while the average work week was a few hours,
though this varies by task.
The very sorts of things that are hard to
automate are the sorts of things folks tend
to find mentally challenging and satisfying,
so I could easily see a society saying “Things
are good enough, more automation via more
intelligent automation is a bad idea.”
Then, either halting AI research, or shutting
off what they had and stepping back to what
they felt was the optimum safe level of automation.
Again, I can’t see them trashing all their
computers and robots.
They might feel obliged to regress quite far
as well.
As an example, if a civilization suffers heavily
from a super-plague engineered by some small
cabal of people, they might feel the best
way to prevent that happening again was to
reduce their relevant technology to below
what was necessary for some smart and evil
group to produce one again.
Still, it doesn’t seem likely they’d trash
all their hospitals and biolabs, even if they
overreacted.
In the same vein, someone might invent really
good 3D printers that could spew out weapons
of mass destruction in some lunatic’s basement,
and society might feel the safest thing was
to abandon that technology.
More likely, they’d limit its use to guarded
industrial applications where it was uniquely
beneficial, and slap on all sorts of security
on the hardware and printer templates.
We do have a lot of experience with super-dangerous
technology after all.
Based on our previous brushes with it, I wouldn’t
think we’d be prone to go overboard on restrictions
to the point of abandoning any tech related
to the menacing one for fear some genius lunatic
might be able to cobble that technology back
together from it.
In a scenario like that, a Post-AI society
is hardly a post-technological one, it’s
just one that limits AI.
Which AI it limits, and to what degree, might
vary a lot.
They might declare nothing capable of even
passing for smart mammal intelligence was
okay, or they might say even human-level AI
was fine, but any super-intelligent AI was
banned or had to be kept under secure lockdown
for emergency or controlled use.
As an example of that, if you’ve got a super-intelligent
machine, you might keep it shut off and quarantined
specifically to be turned on if something
awful hits the fan, like another super-AI
emerging in spite of your safeguards or some
unexpected cataclysm you can’t handle, like
detecting a rogue black hole headed toward
our solar system and not knowing what to do,
in which case you’re gambling possible destruction
at the hands of AI who might help you with
the problem versus certain destruction, that
sort of scenario.
As to keeping one to deal with another AI,
well, it’s probably easier to keep an AI
under secure lockdown with expert control
on hand than to monitor a whole world for
some clever idiot who managed to brew one
up at home that got loose, in which case you’ve
got that gambler’s choice again.
But you could easily have lots of nations
that kept them around for fear of that scenario
or their neighbors being willing to unpack
their AI if they were in desperate need to
use it against you, similar to the policy
many have toward conventional weapons of mass
destruction and the MAD, or Mutually Assured
Destruction philosophy.
Incidentally that doesn't necessarily mean
that AI is running, it is a computer after
all, you could keep it switched off with a
lot of its components stored separately, only
to be recombined and switched on in an emergency.
That represents a time lag but much like having
launch protocols for ICBMs, you’re likely
to conclude that’s a decent option over
leaving your AI switched on all the time in
case your rivals use theirs.
That’s not exactly a post-AI civilization
since you have AI, but it’s parallel and
strikes me as plausible.
You keep one around just in case someone else
let’s theirs loose, so that it can fight
them.
Again, the parallels to nuclear weapons are
a bit haunting.
Note that this isn’t necessarily a military
engagement either, such AI could be great
at trade wars or research races, and I suspect
that would be their preferred deployment anyway.
You don’t really need super-intelligent
war drones, such things are likely to be more
smart-insect level, possibly with a smarter
hive intelligence controlling a local swarm,
not a global network… see Attack of the
Drones for more discussion of how high-intelligence
isn’t necessarily beneficial for rapid battle.
Rather you need the super-intelligent machines
for advice and strategy, be it on military,
economic, or research matters, but that doesn’t
necessarily require giving it the keys to
your bombs, factories, or stock markets.
Of course, you might worry that a bunch of
those AI, if unleashed, might team up together
rather than fight each other, which is certainly
possible but raises some problems with motivations.
As an example, if you got a bunch of megalomaniac
super-minds bent only on personal survival
and power, they don’t really have any motivation
to forge alliances with each other, and may
feel no kinship to each other either.
The enemy of my enemy is my friend is a truism
that generally assumes alliance with a lesser
threat, such megalomaniac AI might be more
comfortable allying to humanity than to a
rival AI, seeing us a able to tip things in
its favor against that rival than able to
deal with us afterwards if necessary.
Needless to say, they might be very good manipulators
too, and we’ve a bad habit of assuming they’d
lack social skills when in practice they might,
via their super-intelligence, be incredibly
charismatic and persuasive.
You don’t need the keys to the guns and
money if you’ve got the ear of those who
do.
More importantly though, you also need to
fear the folks who have those keys, and the
keys to the AI, other humans who wanted to
use them for personal power and dominion,
and as mentioned earlier that’s another
plausible jihad scenario for going post-AI,
to prevent not rule by machines but rule by
those who rule the machines.
So what does a post-AI society do?
Well, especially if you already used them
for technological development and basic science
and concept innovation, you probably can keep
using many of the non-AI technologies developed
by those AI.
It might take a genius to develop an idea
but much less genius is needed to replicate
it.
Again it depends on how much automation they
want to keep and also why they abandoned the
AI.
If, for instance, they just wanted only human
minds, not advanced computers, they might
decide to raise the bar by making smarter
people, akin to the Mentats in Dune.
It’s quite possible a civilization that
abandoned AI might also ban genetic engineering
and cybernetics, particularly mind augmentation,
but that too might not be all or nothing,
with some allowed but some not.
We don’t yet have AI that represents a big
threat in the classic sci fi sense, but we
do have crude AI the behavior of which can
already be troubling at times.
We use it for gaining information about people
from giant stacks of data on seemingly unrelated
things, and so while we would initially say
that our current level of technology is fine
against threats like Skynet, a society might
feel that letting corporations or governments
be able to do super analysis was a bridge
too far.
Indeed, I imagine we’ll see more and more
backlash against that in years to come, as
it improves, and people better know what that
capacity really is.
That might not result in bans, but I could
see it resulting in a ton of regulation and
restrictions.
Too much capability to predict the behavior
of people or the future might be seen as a
very bad thing, and amusingly that is also
a concept explored in Frank Herbert’s Dune
series in regard to precognition.
In the end, if we ask ourselves what society
would be like after AI, I think the answer,
for good or ill, is that we’d never find
out, because we’d never completely get rid
of it unless we had compelling evidence that
any of it existing at all was guaranteed to
result in disaster, and the way you’d get
that evidence would generally exclude anyone
being around and able to make and enforce
that ban.
After all, it is hard to ban a malevolent
super-intelligence capable of ending the world,
if it’s already proven it’s malevolent
and capable of ending the world.
So we were talking today about artificial
intelligence and if you’re curious about
AI, Neural Networks, and other computer concepts,
there’s a number of amazing courses on this
topic at Brilliant.
Neural Nets are one of the expanding fields
of computer science and Brilliant has an excellent
course on the topic.
Brilliant’s focus on fun and interactive
methods makes them a great choice, whether
you’re a student, a parent trying to enhance
your kid’s education, a professional brushing
up on cutting-edge topics, or someone who
just wants to use this time to understand
the world better, you should check out Brilliant.
Try adding some learning structure to your
day by setting a goal to improve yourself,
and then work at that goal just a little bit
every day.
Brilliant makes that possible with interactive
explorations and a mobile app that you can
take with you wherever you are.
If you are naturally curious, want to build
your problem-solving skills, or need to develop
confidence in your analytical abilities, then
get Brilliant Premium to learn something new.
Brilliant’s thought-provoking math, science,
and computer science content helps guide you
to mastery by taking complex concepts and
breaking them up into bite-sized understandable
chunks.
You'll start by having fun with their interactive
explorations, over time you'll be amazed at
what you can accomplish.
If you’d like to learn more science, math,
and computer science, and want to do it at
your own pace and from the comfort of your
own home, go to brilliant.org/IsaacArthur
and try it out for free.
As a quick sidenote, for folks who prefer
to listen to the episodes, as opposed to watching
them, they’ve all been available for free
download on Soundcloud for some years now,
but somewhile back I added them to iTunes
and by popular request they have also now
been added to Spotify, so if you make use
of either now you can get all the SFIA episodes
to listen to and get notified when new episodes
come out.
Speaking of new episodes, this Thursday we’ll
be looking at the idea of Space Police, both
the near term as we get into orbit and colonize
our solar system, and some far future scenarios
like Galactic Police, as well as some past
scenarios, like Time Police, along with some
of the more peculiar crimes the future might
include.
The week after that we’ll explore Graphene,
the super-strong material that might have
an enormous impact on our civilization and
permit the creation of some truly enormous
space habitats.
Then we’ll close out the month of June with
our Monthly Livestream Q&A at 4pm Eastern
Time, Sunday, June 28th.
If you want alerts when those and other episodes
come out, make sure to subscribe to the channel,
and if you’d like to help support future
episodes, you can visit our website IsaacArthur.net
to donate to the show, or become a patron
for the show over on Patreon, and both of
those are linked in the episode description.
Until next time, thanks for watching,
and have a great week!
