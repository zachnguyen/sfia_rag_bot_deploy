In the previous video we outlined all the
major solutions to the Fermi Paradox.
If you haven’t watched that yet, this video
is meant as a stand alone, but it would probably
help to watch that first.
As a reminder, the Fermi Paradox is the apparent
contradiction between the seemingly high probability
of alien civilizations existing and the total
absence of any solid evidence that they do
exist.
Or as Enrico Fermi put it, if there are tons
of alien civilizations, “Where is everybody?”
We outlined a lot of proposed solutions in
that video from us just not knowing what to
look for to find them to them going out of
their way to hide themselves or us just refusing
to believe evidence right under our noses.
However one of the more popular solutions
revolves around the idea that alien civilizations
are terribly uncommon, either because they
rarely develop civilization or because they
almost always go extinct.
Today we’ll be looking at the latter option.
If you’re alive, today, in the 21st century,
and watching Youtube, then you have already
been exposed to virtually ever apocalyptic
scenario imaginable.
Hollywood does a wonderful job blowing stuff
up for our entertainment.
We’re going to go over some of these and
see how realistic they are.
But we’re not interested in some of these
cases because they don’t really matter in
terms of the Fermi Paradox.
For instance, portrayal of genetic superman
wiping out mankind is really an irrelevancy
as far as Fermi’s Paradox is concerned because
highly aggressive genetic supermen are just
as likely to go expanding out into the wider
universe as we are.
Doomsday’s for humanity that replace us
with another intelligence that has survival
and propagation as motives just don’t matter
for Fermi’s Paradox.
And as we previously discussed in the Dyson
Dilemma video even a singular intelligence
which didn’t want to reproduce has good
motivations to expand itself.
We’re looking for something that ends civilization.
Not civilization as we know it, but just destroys
any intelligence with the desire and capacity
for expanding into space.
So for instance a swarm of self-replicating
robots with the technology for interstellar
travel but nothing approaching a civilization
is a Fermi Paradox concern, alternatively
a complex and ancient civilization which arose
after a nuclear war and bans any 20th century
technology and enforces those bans rigorously
is not.
One could argue in that latter case though,
that anyone who bans technology because of
some cataclysmic event is going to have problems
enforcing that over very long times.
Societies mutate on a generational basis,
and especially without solid worldwide communications
you’d expect to see a lot of schism and
fracturing.
Even keeping an orthodoxy for a thus an years
seems hard, but planets exist for billions
of years and the simple fact is that, all
things being equal, a group which starts ignoring
the tech bans are going to start gaining a
massive economic and military edge.
So we’re going to go over a lot of these
doomsday scenarios and discussing not so much
how likely they are to occur but how likely
they are to take a civilization down entirely
for keeps.
It’s hard to really classify civilization
destroying events but we are going to give
it a go.
Here’s the one’s I’ve selected are:
Nuclear War
Bioterrorism
Post-Apocalyptic Attrition
Cyclical Apocalypse
Climate Change
Artificial Intelligence
Grey Goo
Anti-Matter
Suicide Pact Technology
Mind Control
One thing we’ll be mostly avoiding is social
matters.
Not only are these hard to predict, and as
we pointed out, prone to very rapid change
on stellar timelines, but they get pretty
controversial because they get political.
Everyone’s mileage varies on such things
and there’s not really a rigorous approach
available to analyze such things, so we’ll
just leave it be so the comments section under
the video doesn’t get full of invective.
By the way I do read all the comments on these
videos and try to reply to them, so please
feel free to leave a comment below.
So, Nuclear War.
This is one we’re all pretty familiar with
but like a lot of disasters in movies it tends
not to get portrayed very accurately.
Nuclear bombs are devastating, but fundamentally,
even at the height of the cold war, there
just weren’t enough to kill off civilization
for keeps.
Nuclear bombs kill many, collapse present
society, and leave long-lasting damage to
the ecosystem.
However, they don’t get everybody, and in
most estimates don’t even get half.
Starvation and chaos will get far more, but
ironically are less of a threat the more people
die initially.
We’ll cover this more in the Post-Apocalyptic
Attrition segment but it basically boils down
having some surviving settlements that eke
their way through the centuries it takes for
the planet to recover.
Fundamentally, just as with our own Dark Ages,
these periods simply are very short compared
to the timespan planets exist for.
If some world had beat us modern civilization
by a mere million years, an eyeblink compared
to the Age of the Universe, even if it took
them a thousand years to recover from a nuclear
was and even if they repeated the process
10 or 12 times before learning better, or
running out of uranium, they’d still have
burned up only a percent or two of their headstart
on us.
Now a sufficiently large nuclear device, or
nuclear arsenal, could kill everyone.
We can’t rule this out but there’s a host
of hurdles to building an arsenal that’s
simply far bigger than you need to destroy
your enemy ten times over again.
Nukes are expensive to build and maintain,
arsenals on that scale are basically deadman
switches designed to convince an enemy that
attacking is suicide, people are very interested
in their own survival and would tend to build
in massive safeguards.
We never built such things, odds are good
few others would either and the Fermi Paradox
is all about the odds.
If 1 species does this but 9 others don’t,
it doesn’t really matter.
So fundamentally nuclear war, at least with
the technology so far devised, isn’t a terribly
good Fermi Paradox solution.
So what about biowarfare?
The same conditions apply to any virus that
doesn’t get everyone, but we can’t rule
out someone designing a virus that does get
everyone.
And it really does need to get everyone.
If even 1 in a million are immune that still
leaves thousands of people alive.
They’d be scattered all over but since vast
amounts of technology and resources would
be kicking around intact it would only be
a matter of time before lonely people started
making efforts to link up with each other.
When you’ve got months or years to contemplate
the problem you don’t need to be a genius
to think of lighting beacon fires on top of
skyscrapers or using a radio tower to broadcast
to others.
And that’s assuming no one had enough time
during the crisis to start taking measures
like telling the survivors how to link up
or get people into a sealed environment with
resource stockpiles.
And if the immunity was more like 1% you wouldn’t
even need special measures to link people
up into groups big enough to survive and breed.
So it would have to a 100% kill.
And that’s not really a natural or accidental
lab accident sort of result.
You’d almost have to have a large and well-funded
group set out to intentionally kill every
single person on the planet.
Outside of Hollywood that’s not terrible
tenable.
The bigger the group, the better the odds
of someone changing their mind and informing
authorities.
Even if we grant all that though, we still
have three problems in the context of the
Fermi Paradox.
First, not every alien species is likely to
do this.
Just like with nuclear war, it doesn’t matter
if 1 in 10 does if 9 in 10 don’t, and even
if it were 9 in 10, or 99 in 100 that did
this it still wouldn’t really matter much
to the Fermi Paradox because if you have a
thousand or so civilizations arising in a
billion years per galaxy even if 99.9% did
this to themselves you’d still have one
left over who didn’t.
Second, when we’re working in the context
that intelligence tends to develop fairly
commonly, then wipe itself out, which is what
Doomsday approaches to the Fermi Paradox revolve
around, then wiping out all humans with a
virus would still leave other intelligent
mammals around who would, if intelligence
evolving is common, replace us.
It might take a million years, or ten million
years, or even a hundred million years but
even a hundred million years isn’t that
long compared to the age of the Universe or
this planet.
It’s around 1%.
Third, we also have to remember that viruses
might not really be a universal threat.
Viruses are present in, and a threat to, every
environment and organism on Earth.
But that doesn’t mean they’d be an automatic
threat in all life-bearing worlds to the degree
they are to us.
A given species might have much more effective
immune systems or a biology which is just
very toughened against dying from infection.
So this type of apocalypse also isn’t’
a really good Fermi Paradox solution.
This takes us to Post-Apocalyptic Attrition,
which is less of a specific Apocalypse and
more about recovery.
One common theme of disasters is that you
often get more damage done in the aftermath
than the actual incident itself.
In fiction in particularly we’ll have rioting
mobs or in the aftermath roving brigands who
basically steal to live and destroy any stable
bastions of civilization they encounter.
This results in a steady attrition just pushing
us down till no one is left.
Now the problem here is you can only kill
so many people before theft-to-live is no
longer viable.
Once your numbers get down to hunter-gatherer
levels, a few square miles per person, it
now is not only no longer necessary to kill
people to get food but also a bad strategy.
Deer won’t shoot you, and if all that’s
left of civilization are the people who’ve
survived the bloodbath odds are good those
survivors can and will shoot you.
You’ve also got the problem that an organized
and disciplined group that’s been keeping
around people who have other values besides
fighting, like repairing stuff and treating
injuries, is going to butcher the stereotypical
post-apocalyptic Mad Max tribe of techno-savages
in a fight.
So eventually attrition slows down, and it
is basically bound to do this while the numbers
are still high enough that people still regularly
encounter each other so you really can’t
push a population down below breeding-level
viability through attrition.
After a while, even if they’ve lost all
their technology, and especially if they lost
it all, they just revert to hunter gather
tribes.
A setback, but presumably of thousands of
years at most.
And it would probably be a lot less, because
examples of technology, rusty or not, are
going to be lying around everywhere and you
can fit pretty much every major bit of science
and technology up to the 20th century into
books which would take up a small chest.
Heck you can download the entire Wikipedia
database free of charge onto a single thumb
drive.
But you’d be bound to have caches set aside
by people of this variety and even if you
didn’t you don’t need to be nearly as
smart as the guys who designed the internal
combustion engine to reverse engineer it from
some rusty remnant.
Our own buried landfills, unlikely to be pillaged
in an apocalyptic aftermath, would contain
everything you needed.
So you would expect civilization to recover
eventually, and sooner than later, in any
doomsday which left enough of us alive to
breed and was still ecologically intact enough
to support life.
But having killed themselves once, what’s
stopping them from doing it again?
Cyclical apocalypses, natural or unnatural,
are a real concern.
We’ve got some great examples of this in
fiction (Pern, Nightfall, Mote in God’s
Eye) and of course these being fiction tend
to either focus on breaking the recurring
cycle or if more bleak and dark, on the futility
of trying breaking out of the cycle.
But how probable is this?
That’s really hard to say.
If a civilization arises, nukes itself nearly
out of existence, recovers over thousands
of years, then does it again, what’s to
stop them doing it again, and again, and again?
Well I mentioned half-jokingly early you’d
eventually run out of enriched uranium to
build nukes with.
Radioactive isotopes decay with time, that’s
why they are rare to begin with, not much
left over from the ancient supernovas that
spawned them, and if you’ve being mining
every source of them for cycles of nuclear
warfare you will run out.
But fundamentally its really hard to eliminate
all traces of a civilization.
Even if every single historical account is
burned or bastardized into mythology that
barely resembles what really happened you
going to start having increasing amounts of
archeological and even geological records.
It would be hard to miss evidence that every
ten thousand or so years for the last couple
million years civilization had gotten technological
then knocked itself back to primitive hunter-gatherer
remnants.
And even that’s a pretty unrealistic scenario.
We really do have bunker-archives, even back
to the Crypt of Civilization built in Georgia
in 1936, which would be usable guides to reforging
civilization even thousands of years after
they were built.
These sorts of time-capsules and caches probably
would survive almost any doomsday scenario
that left people alive.
If you’re pessimist you can come up with
ways these might fail to survive or just be
disbelieved but intelligent critters are usually
very survival oriented and it’s hard for
me, personally, to imagine such cycles just
going on and on uninterrupted for thousands
of cycles till the world got burned up by
the sun.
Our next one is climate change.
This is politically controversial so we’re
going to keep this short, like I said I like
people to leave me comments on the videos
but I don’t invite invective.
Please take pro or anti climate change arguments
elsewhere, plenty of other places to talk
about that.
Global warming for this is pretty straight
forward anyway.
It is possible climate change could leave
a world dead, rendered either as a snowball
Earth or Runaway Greenhouse Venus setup.
We don’t know.
If 
this is possible then everybody’s dead and
it’s a decent Fermi Paradox option.
Except that it would require every species
does let this happen and that’s counter-indicated
because even if they all would, its not really
a given all could.
Our fossils fuels are the product of a very
long and complex geological process.
Underwater oil deposits often rupture and
leak out, underground coal seams can burn.
So there’s really nothing saying every world
would have as much of these items as we do
when technological life arose.
They might be immune to the problem because
they don’t much.
Plus the individual configurations of the
planets, its distance from the sun, its type
of sun, i’s own default atmosphere, and
so on, might make it immune to such problems.
They might live on a world with a dozen times
more atmospheric carbon then we already have
and that’s what lets it be warm enough for
life.
Their world might be very prone to earthquakes
compared to us and all the easy deposits of
fuels ruptured.
Add to that, if it wouldn’t wipe out all
life but was recoverable over time, then civilization
would recover and wouldn’t have those deposits
for technology round 2.
Now that’s a problem because we used those
to get to our present technological state,
but we did have renewable fuels before we
used these, like alcohol or ethanol, firewood,
and so on.
You can’t support as many people in the
short term, and fuel is more expensive, but
this probably would just slow technological
process, not halt it.
And again centuries or even tens of thousands
of years mean nothing in the timeline of the
universe.
So this also isn’t a good Fermi-Paradox
solution.
Our next one, Artificial Intelligence, also
really isn’t.
As we mentioned earlier and in previous videos,
replacing one intelligence with another doesn’t
mean anything to the Fermi Paradox unless
its motives are so different that it doesn’t
act in any way which would emulate either
of the three basically universal rules of
natural life.
That it very focused on surviving, that it
wants to reproduce, and that it wants to expand
its resources and territory.
An AI which didn’t care about tis own survival
isn’t likely to replace the species that
birthed it, and we discussed in the Dyson
Dilemma video why even one which didn’t
want to reproduce itself would still tend
to functionally emulate an expanding and growing
species.
So AI really only matters in the sense of
being an apocalypse which kills it and us.
The big problem with this is that an AI which
is smarter than us, a Strong AI, is smarter
than us.
In Hollywood these AI’s run numbers quickly
but are generally absolute morons when it
comes to social interaction.
A real strong AI probably isn’t going to
scare us because it will be smart enough to
emulate a very pleasant, helpful, and non-threatening
friend.
An AI planning to take over the world is probably
a lot more likely to get the courts to rule
it legally a person and convince people to
vote it president in landslide election then
to go all SkyNet or Matrix on us.
If it wants us all dead, we’d probably never
know it till it acted and the outcome would
be about as predetermined as fighting off
an alien invasion by a species set on genocide
and capable of interstellar travel.
Which is to say the ‘battle’ would last
about ten minutes.
Which is to say, you don’t beat a smarter,
bigger, more technologically advanced group
in a one-on-one fight unless its goal isn’t
to kill you in the first place.
If it just wants you dead, and doesn’t care
about the property damage, an interstellar
ship doesn’t even need to fire any guns.
It just has to jettison some of its mass aimed
at Earth before it starts slowing down.
Because a few thousands tons of garbage moving
at relativistic speeds and hitting Earth would
make a nuclear war look like a barroom scuffle
in comparison.
A machine doesn’t need an intact biosphere
to live so it can go all scorched Earth.
Hard to beat an enemy who is smarter than
you and has a much freer hand where collateral
damage is concerned.
So it’s really unlikely a fight with a machine
mind would end in a mutual kill, and the survivor
would pick itself up and recover, not a good
Fermi Paradox option.
Alternatively a stupid machine is.
Grey Goo as its often called, microscopic
little machines which are simply programmed
to tear everything apart to make more of themselves,
is an apocalypse scenario which fits into
the Fermi Paradox.
Though only if they’re too dumb to make
it off world.
This is a lot like the biowarfare option only
far worse because there’s no immunity to
being disassembled as scrap by a billion little
robots who also disassemble all the animals,
vegetables, and minerals.
So Grey Goo is a real concern.
Two caveats in the context of the Fermi Paradox
though.
First, there’s no reason to assume that
even if Grey Goo starts off mindless it would
stay that way.
We, ourselves, are essentially descended from
Grey Goo.
Early life just mindlessly ate anything it
could to reproduce.
It mutated and grew in complexity.
Grey Goo would also likely be capable of mutation,
it’s basically unavoidable in replicating
life or machines.
Now Grey Goo reproduces a lot faster than
biological life, if it didn’t it wouldn’t
be a threat.
So it might climb the evolutionary chain to
intelligence much faster.
Of course it might be exponentially less prone
to mutation too which might cause the reverse.
But from a certain light Grey Goo is just
a life reset akin to when oxygen breathers
became the norm for life on Earth.
The other caveat is that self-replicating
machines are probably not the boogey-man we
make them out to be.
Just because something can replicate faster
than biological life doesn’t mean it can
replicate absurdly quickly.
There’s waste heat involved in any mechanical
process.
Try bending a metal coat hanger back and forth
several times quickly till it breaks and sticking
your finger at the snap point.
You’ll burn your hand.
They’d be limited to how fast they could
rip stuff apart without melting themselves.
Second, you’re probably aware that electronic
devices are sensitive to EMP bursts.
You can harden electronics against this but
only to a point.
Especially with tiny machines adding in all
that shielding is just going to slow its ability
to reproduce down because now it needs more
resources for each new machine and more time
to make it and fewer can cluster together
for each disassembly.
If they’re dumb, you can fight them, if
they’re smart, they’re wired together
as a group consciousness, which makes them
smart and thus not a Fermi Paradox solution,
and also makes them sensitive to signal jamming
too.
So you probably can’t end the world from
someone accidentally dropping a vial of the
things to shatter on the concrete and go on
a blitzkrieg.
You can also nuke them.
Nukes beat matter every time.
Nuclear forces and energies are just orders
of magnitude more powerful then chemical bonds
that bind matter.
That takes us to our next one.
If there’s one thing out their nastier than
nuclear bombs in destructive potential its
anti-matter.
A baseball size and mass of antimatter, dropped
to the ground, would go off with a destructive
blast on par with the largest nuclear devices
we’ve ever made.
They release all their mass energy, whereas
nukes release not even a percent of theirs.
This is exactly what makes interstellar ships
so dangerous and why hard science fiction
fans often chuckle the notion of an ‘unarmed
ship’.
Anything going out at those kind of speeds
is worth more than its weight in nukes in
destructive power because the energies involved
in those kind of speeds parallel mass energy,
Einstein’s famous E-mc-squared.
You could blow up a fair sized city with just
the antimatter that would fit inside a ballpoint
pen’s interior.
So as a Fermi-paradox solution we’d have
to consider what would happen if this substance
became very easy to manufacture and hide.
Right now anti-matter is very hard to make.
As powerful as it is a single gram, the mass
of a pea, is parallel to the Hiroshima bomb.
But it is so hard to make it would take us
a billion years at present production to make
that gram and we would have no way to store
it.
That could change, we could find a way to
produce it and store it cheaply and efficiently
and covertly and that would make it the ultimate
terrorist device.
Now, we’re not doing a separate section
on 3D printing but 3D printing, while a marvel,
comes with some attached concerns.
In theory, when fully developed, you can produce
anything at home if you have the template.
So easy availability by lone crackpots to
anti-matter, to 3D printers that could fabricate
destructive devices, and so on, could make
it virtually impossible to maintain a technological
civilization because it becomes too easy for
lone members to damage it and we have no shortage
of lunatics.
This becomes a possible Fermi Paradox solution
in the sense of species having to step back
because they just can’t maintain technological
infrastructure to support interstellar travel
without being constantly vulnerable to their
own crazy persons wreaking disproportionate
destruction.
Our last category, mind control, will ponder
that some more, but we have one other category
first, and like antimatter it revolves around
very powerful energy sources.
Suicide Pact Technology is essentially any
tech that just by using it guarantees destruction.
Some people say that internal combustion engines
or antibiotics or nuclear power or computers
are these things.
This category in this video though refers
more to technologies that look too good to
be true and turn out to be apocalyptic.
You might remember some years back the concern
people had about the CERN supercollider making
a black hole that would sink the center of
the earth and gobble up the planet.
I’m not going to address all the flaws in
that idea but let’s hypothesize someone
invented some powerplant that sucked energy
out of seemingly nowhere for free.
Unbeknownst to them this really cool device
sets off a ticking time bomb that blows the
planet to smithereens a year after use in
a way no one can foresee.
If the physics allowing this device were almost
impossible not to discover before space travel
was really practical, like how you almost
can’t avoid discovering computers before
you can make orbital satellites, then every
species figures it out, uses it, and blows
themselves up before they can expand into
the stars.
Total Fermi Paradox solution.
And 
again some feel computers, and AI, are an
example of this.
Or fossil fuels.
Or Nukes.
As we’ve been discussing throughout the
video.
Suicide Pact Technology isn’t a specific
apocalypse but the notion of a technology
that unavoidable kills its users and unavoidably
gets discovered soon enough to kill them.
How plausible is this?
Well, like any Fermi Paradox solution we have
to consider if it covers all the bases.
WE haven’t discovered any technology to
the point of definite death yet and we really
could build a Moon Base or Mars Base if we
wanted, probably one that could keep going
even if Earth ceased to exist.
And other alien worlds might have their interplanetary
neighbors be closer, might have lower gravity
or thinner air to make orbital launch easier.
So 
besides us having nor real reason to believe
there’s any Suicide Pact Technology that
can and will always destroy a world there
would probably be examples of critters who
had reached our effective technological level
and already gotten a splinter colony on another
world in their system.
So even this isn’t really a great Fermi
Paradox Solution.
Our last category is mind control.
As we mentioned in the anti-matter section
you’ve got to worry, inside a technological
civilization, about crazy individuals being
able to wreak vast damage.
One solution to that, if you have the technology,
would be to have a total big brother state
or even outright mind control.
And certainly there are even more sinister
reasons someone might want to use this sort
of technology.
Now in of itself this is irrelevant to the
Fermi Paradox.
But specific types of mind control would be.
For instance, a culture that’s gotten burned
by technology might use mind control to permanently
lock technological progress at a ‘safe’
level too low for interstellar travel.
Or, similarly, they might outright ban expansion
because they can’t reliable maintain that
enforced orthodoxy over light years of space
and communication lag.
They don’t trust any colony not to splinter
off and become a threat down the road.
It’s very hard to rule two islands, so to
speak, because one of those islands will constantly
be wanting to break free and maybe that potential
threat just isn’t worth the risk for the
resources it brings you, and interstellar
space isn’t a planet, moving resources is
a hugely long and hard process.
So 
any sort of mind control or social control
that results in motives not to expand and
could reliably keep enforcing that for millions
of years is a valid Fermi Paradox solution
but only where there’s a universal motive,
since it has to be all or nearly all aliens
doing it, and that would rally only apply
where there was a clear suicide pact technology
on the horizon.
So there we go.
In summary, we looked at possible doomsday
scenarios for what might take out intelligent
life and we’ve seen that there’s not many
good fits, in terms of the Fermi Paradox,
for things which would get most let alone
all intelligent species.
I hope you’ve enjoyed this video, fell free
to check out some of my others or leave comments
below.
Thanks for Watching and have a Great Day!
