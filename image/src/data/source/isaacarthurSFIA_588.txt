So we meet an alien civilization that could
solve all of our technological problems,
only the smug jerks refuse to give it to us
because they are afraid it could hurt our cultural development.
So today is this channel’s 100th episode,
and the topic seemed appropriate for the milestone,
or milestones, since this week is also the
third anniversary of the original episode
on Megastructures.
Now the reason a weekly show is celebrating
episode 100 on its third anniversary, rather
than episode 156, is that early on the show
was not weekly, and indeed it was about 4
months before the second episode came out,
discussing the implications of those megastructures
in terms of the Fermi Paradox, the question
of where all the aliens are.
This channel was built with three basic focuses
in mind: megastructures built by high-tech
civilizations, speculating about high-tech
civilizations, and challenging the assumptions
we receive from science fiction and Life,
the Universe, and Everything.
So celebrating our 100th episode and third
anniversary by returning to the Alien Civilization
series seemed appropriate.
Fundamentally, this series is an extension
of our discussions of the Fermi Paradox, but
focuses more on the motives of hypothetical
aliens and trying to see if they make sense.
It’s debatable if the Fermi Paradox should
be called a paradox, since we still know so
little about the Universe to be calling aspects
of it contradictory, but it tends to seem
that way since so many of the suggested solutions
only make sense by discarding some assumption
about Life or the Universe that seems solidly
rooted in common sense.
If we accept intelligent life is incredibly
rare, it feels like a modernized form of geocentrism,
viewing ourselves as somehow special.
If we assume we are not, we have to figure
out why the Universe isn’t flooded in alien
civilizations, or is but we can’t see them.
Since we know we, by our modern view of things,
would absolutely go around exploring and colonizing
the Universe and saying hello to every alien
we met, we need some reason this isn’t happening.
Essentially why modern humanity’s priorities
and outlook would not be a representative
sample of intelligent life in this Universe.
And we’ve gone through tons of suggested
solutions, from all life dying off from technological
apocalypses, to them hiding out, or maybe
being unable to travel the stars, or simply
not wanting to.
None of those offer us the Universe we saw
in film and television like Star Wars or Star
Trek, with tons of aliens and easy interstellar
travel.
The classic Space Opera Universe many of us,
myself included, would love to be true.
Today we'll examine the Zoo Hypothesis, often
regarded as a good Fermi Paradox Solution.
One example of it is the well-known Star Trek
Prime Directive.
I’ll paraphrase it as not interfering in
other civilizations, especially those who
are still technologically limited to their
own original planet.
In Star Trek, there’s a line between civilizations
with Warp Travel and without, for those who
have it, have interstellar travel, you can
contact them openly, presumably on the grounds
they either already know of alien civilizations
or will soon find out.
No contact before that.
Now, before we continue, I should add that
it is really the Zoo Hypothesis we are looking
at today, and that extends beyond the example
of the Prime Directive.
But we will give that a little extra focus
today as it is more familiar ground.
So we want to ask ourselves why someone might
choose not to interfere in lower-tech civilizations,
or even wall them up in the equivalent of
a galactic zoo.
What’s their definition of non-interference?
When and for what do they make exceptions,
if any?
And how far will they go to enforce the policy?
The key aspect of non-interference policies,
in regard to the Fermi Paradox, always comes
down to enforcement.
You are a Starship captain observing a planet
full of primitives, you aren’t interfering
in their civilization, and fair enough, I
don’t consider non-interference always a
preferred option, but it’s decently ethically
sound as a basic principle, keep your nose
out of other people’s business.
We often do this when observing animals in
the wild or reporting on news, minimal interference,
and we have a variety of motives for such
policies.
It depends a lot on what your basic motivations
are for doing it, and a lot of time in Star
Trek it doesn’t seem to be the best interests
of the civilization not being interfered with,
since they’ll stick to the policy even when
the alternative is that civilization being
destroyed.
A friend of mine often referred to those episodes
as Smug Trek, since they seem too confident
and superior in their non-interference even
when there seems no way to argue it benefits
the native species, and they always get rescued
from the consequences by some plot contrivance,
hence part of the reason for the episode title,
Smug Aliens.
The other half of that being alien civilizations
in fiction who tend to qualify as “Space
Elves”, the loose nickname for when a writer
creates a race that is supposed to be ancient,
wise, and enlightened, but come off more like
smugly superior jerks.
Frequently for refusing to help directly,
or through sharing technology in dealing with
some horrible galactic menace because they
have some sort of non-interference policy.
One that for some reason always seems to mysteriously
exempt giving lectures about how the primitive
Earthlings should behave.
And yet, the test of any given guideline or
rule is how it functions in extremities and
strange cases.
You are sitting there observing this species
in their stone age and know that you can’t
talk to them, by your own rules, for several
thousand years to come.
That’s the first problem; how realistic
is it that you can avoid contact that long
while monitoring them?
Can you really expect to go centuries with
dozens of folks working on such a project
without one of them messing up, or even doing
it on purpose.
It is after all, pretty hard to watch a civilization,
or rather a group of civilizations, regularly
get smashed up by disasters, plagues, starvation,
or belligerent neighbors and not do anything,
when you could.
These aren’t strangers either, you are getting
to know them even if they don’t know you.
And if you know that over all that time someone
is likely to break the rule, is there much
point to even trying to observe it?
Someone is bound to ask what the real point
is?
I mean do the aliens below really need to
learn the hard way that plagues are bad and
that hygiene is good?
You might say that without those they’d
never develop medicine, but who says they
need to?
I’ve never invented a vaccine or suffered
from a plague, and I don’t particularly
care if the guys researching new vaccines
learned the trade from guys who did or aliens
who cured all theirs a million years ago.
Civilizations don’t invent technology, individuals
or groups do, yet we still assume the rest
of the civilization ought to be able to benefit
from that, we don’t expect every person
to learn microbiology before taking antibiotics.
And yet, most of us do tend to either think
non-interference is a good general policy
or if not, we respect the basic concept.
In general, ‘leave those folks be to find
their own way’, is a philosophy we can respect.
Where it goes a bit overboard is in the specifics.
You are monitoring some civilization and detect
a giant asteroid en route to hit the planet.
Do you take any action?
If yes, or no, what’s the threshold?
Is it okay to knock aside one that would sterilize
the entire planet but not one that would just
kill most people?
If so, why?
If yes to both, what about one that will just
hit one city and keep the damage pretty localized,
like one about to fall on New York City or
Ancient Rome?
And if yes to all of the above, why not help
with a plague?
And what about the difference between one
that will kill a few people and one with a
genuine 100% mortality rate and airborne to
boot.
On top of that, does it matter what those
aliens want?
I mean I would happily accept a cure to cancer
from aliens, and if they said “Isaac, we’d
love to give this to you but we are worried
about the cultural damage our involvement
will cause.”, my reply would simply be “Let
us worry about that, it’s not your concern”.
But there is obviously a line on that too.
I’d be happy to have their technology, but
I don’t want their opinions on how we should
run our economies, which type of government
we should use, or what sort of things we should
or should not outlaw.
At the same time, I can hardly tell them they
can’t have opinions on such things and share
them when asked, and we all know that whether
or not their opinion on something really is
more enlightened, tons of folks will view
it that way and use “Ah, but that’s how
they do it, and they are older and wiser”
as their argument.
So it’s not a clear-cut issue, but let’s
go back to our asteroid case.
We will say it is a planet-killer, when it
hits, it’s going to sterilize the planet.
The captain calls his officers together and
asks them what to do.
The XO says no way, the policy is clear, we
do nothing, it’s sad but rules are rules
and if we break it on this, what next?
Maybe we go cure their diseases and teach
them to make fusion reactors, and fusion bombs
too?
For all we know they might turn out to be
the next species of genocidal lunatics that
will sterilize other people’s planets.
The engineer says maybe that’s exactly what
they should do, this policy is monstrous,
and that reasoning is no better than not helping
a kid out of burning house on the theory they
might grow up to be a serial killer.
These guys have maxed out their brains from
an evolutionary standpoint, same as a stone
age child adopted through a time machine to
21st century Earth could learn the technology
as well as a child of that era could.
Why not just give it to them now, and give
them the benefit of our wisdom about what
to do or not to do?
Blow up that asteroid, land, introduce ourselves,
and share our knowledge.
The XO is aghast at this, of course, and the
captain doesn’t approve either, so the science
officer says, look, we don’t have to go
all the way on this, we just blow up that
asteroid, this policy is meant to protect
them from us, and an extinct civilization
doesn’t need protection.
If something truly threatening comes up again,
we’ll decide at that time what to do.
Now in your typical TV show, looking to avoid
morally ambiguous plot resolutions, this would
be where the science officer says “Captain,
while I was calculating the minimum deflection
the asteroid needs, I realized that the gravity
of our own ship had perturbed the asteroid,
if we weren’t here it wouldn’t hit them”
or maybe “Captain, this asteroid isn’t
natural, this is actually an artificial asteroid
clearly sent by ‘the bad guys’ to look
natural” and the captain can confidently
order the asteroid dealt with and everyone
is happy and forgets about the unresolved
issue they just had.
If that doesn’t happen, we might see the
captain reluctantly agree to destroy it and
the science officer go to punch in the coordinates
to blow it up only to have the XO pull out
a pistol and tell the captain they’re relieving
them of duty, or even shoot the science officer
and the captain.
Then a three-way fight breaks out ending with
the chief engineer sabotaging the ship to
crash into the asteroid and fleeing before
the impact with the surviving crew members
in the Interventionist camp.
That’s the problem with sincerely held beliefs
on issues involving life and death, people
tend to feel okay about killing for them,
I can’t really call the XO or engineer wrong
for doing what they did.
Of course, the engineer and the other interventionists
now need to decide what to do when they land,
and they need to consider what the response
is going to be from back home when they get
the news in a few centuries.
Back at central command, when they do get
news, they have a few options.
Of course option 1 is that they might not
care, policies do tend to change over centuries,
which is another issue with that mission to
begin with.
Still on option 1, they might still have that
policy but have had so many people break it
in the centuries since they enacted it that
they’ve pretty much given up on enforcement.
Now as to enforcement, what should they do?
Go there and arrest the interventionists?
I might do it as a high-tech civilization
so the original folks might still be alive
centuries later when a new fleet arrives.
But what about their descendants?
Regardless of whether the originals are still
around, can you arrest their descendants?
Can you forcibly deport them?
No other punishment just remove them?
If they resist can you kill them?
What about the original civilization?
Do you take their technology away?
Or repeat what the asteroid would have done
and nuke the place from orbit?
Not many people would be okay with the latter,
I hope, but that’s the only one that really
has teeth as a deterrent.
If you believe letting that civilization die
from an asteroid strike is wrong, odds are
you will take action regardless of whether
or not it means a prison term or even death,
so your only deterrent is knowing it would
be all for nothing, that the armada is going
to come by and torch the place and reverse
what you did.
And of course if the interventionists think
that is a possibility, they might still do
it, and gamble on the chance that in the centuries
they have before word gets home and a fleet
arrives, they can bootstrap the local civilization
up to the point they have a chance to resist.
Indeed, considering the alternatives to not
working fast enough, they have an excuse to
outright play gods to establish and maintain
enough control over the local population to
get everyone working on increasing their numbers
and accepting the new technology and turning
over every bright kid to them for science
and engineering educations.
One might even argue it is better to play
false gods to save a civilization than let
it be wiped out.
They can even rationalize that they’re going
to turn themselves over after the crisis for
judgment, content to pay for that deception
with their lives if that’s what it takes.
What we don’t see in Star Trek or fiction
following a similar code is the Enterprise
firing on some ship that is headed for a primitive
planet with the intent to give them technology.
Considering what we know of that Universe,
anyone sufficiently determined can get their
hands on or build their own warp-capable ship,
and I assume they don’t classify their discoveries
of new civilizations.
So someone back on Earth who disagrees with
the Prime Directive can go replicate or requisition
parts for a spacecraft.
What would they do to a group of folks who
were building a ship and flat out said they
plan to fly to the newest discovered planet
with primitives and say hello and tell the
aliens how to build stuff?
Do they arrest them?
Confiscate the ship?
Quite possibly.
What if they don’t say what the ship is
for, do you warn them off and shoot them down
while approaching the planet?
Do they keep a fleet around each such planet
for such occasions?
What about aliens not in the Federation?
Do they shoot down an alien research team
from a species that doesn’t follow the Prime
Directive?
This is why it doesn’t work well for the
Fermi Paradox, because we cannot expect every
civilization to follow such a principle, and
we can’t expect all their members to either,
and it’s hard to imagine how you would enforce
it over thousands or even millions of years.
Is there some graveyard of ships floating
around our solar system where aliens tried
to run the blockade to help or exploit us
and got shot down?
Probably not.
Let’s assume there was though, and that
normally when you get interstellar capable
they come in and say hi.
When and how should they do it?
Most of us would say that specific technology
is a trifle arbitrary, a simple nod to pragmatism
rather than assuming it implies the civilization
is somehow more immune to cultural contamination
by having it.
We often see in fiction the enlightened race
showing up to talk to a civilization about
our technology level, and usually quietly.
This brings up the issue of whether or not
it’s okay to introduce yourselves a bit
early if you know that they might be about
to kill themselves off in the next 20 years,
but will almost certainly make it to interstellar
travel in the next 10-30 years, and you can
have the same argument as before.
The XO says no, the science officer suggests
sneaking them some vital piece of tech by
email, a design for cheap and easy solar panels
and batteries for instance, and the engineer
just says screw it, call them up and introduce
yourselves and offer them the technology one
generation early.
In a case like this, the captain might be
more inclined to go with the direct and open
intervention route, or at least try a secret
introduction to their leaders.
Of course we don’t know how aliens will
view interacting with other civilizations,
the flaw of the Zoo Hypothesis and Prime Directive
from a Fermi Paradox standpoint comes from
it being very hard to see all of them sharing
the same view, or anyone being willing or
able to enforce it on those who did not.
After all, we do have professional policies
about how zoologists or news reporters are
supposed to act when in the field, not interfering
just observing, but it’s not like that’s
actually enforced.
So it would seem a Fermi Paradox solution
that can’t be valid because it is unlikely
to practical to enforce it.
However, the basic notion of the Zoo Hypothesis
is actually a little more subtle.
Because what folks often miss is that the
Fermi Paradox assumes we are looking up at
the actual Universe and seeing it empty when
it isn’t.
Unless by some freak coincidence all the high-tech
alien machinery and empires are conveniently
invisible, which would be rather weird, you
don’t make a zoo for primitives by limiting
your own civilization, you do it by creating
a fake environment around them.
When we build a zoo near a city, we don’t
deconstruct our skyscrapers, we create a habitat
that conceals those aspects of our civilization
that we need to for their well-being.
So if you are making a zoo out of Earth, you
don’t go deconstructing your own Dyson Spheres,
you make a big one around Earth or our whole
solar system that creates an illusion.
That’s an immense project but way easier
than limiting what you do in the rest of the
galaxy.
And it doesn’t need to be solar system sized
either, I mean it probably isn’t that hard
to snatch up unmanned probes and fake the
data coming back, or even manned missions
and trick the crews.
But that fake Zoo Universe doesn’t necessarily
even need to resemble the real one.
Easier to protect that zoo too, it’s small
and cut off, so you don’t have to worry
about somebody sending a rogue hello signal
into the zoo.
However, that maybe doesn’t go far enough.
We often discuss advanced aliens as being
a sort of higher entity, in the softer kind
of science fiction this is usually some being
of pure energy evolved to a higher plane of
existence or such, but we often just assume
they aren’t organic anymore instead.
We’ll talk more about artificial intelligence
in a couple weeks but for the moment, imagine
some civilization that had gone digital.
They all live in this Universe, or their Universe
rather, but essentially inside computers.
They will consider that sort of existence
just as good as a biological one, probably
a lot more preferable since it is likely to
be vastly more efficient in terms of resources
and energy to support one individual thinking
entity.
That means there is a decent chance they converted
their home planet into one big computer and
just took copies of all the DNA than uploaded
all the animals into it too.
Not just digital people, but digital cats
and dogs and even honey bees.
They could always grow or print up a new organic
body for themselves or those critters if they
needed to, from digital DNA archives.
So from their perspective, what is the best
way to make sure a primitive civilization
is protected?
There’s a good chance the answer they came
up with was to upload them.
Just send in some covert missile carrying
self-replicating machines to land somewhere
on the planet, do some self-replicating underground
while studying the biology for the needed
specifications, then one moment someone is
walking out their front door in the real universe
and maybe stumbles a bit, before proceeding
on, not realizing that they stumbled when
some nano-machines stabbed a spike out of
the ground into their brain and copied it,
and every person and animal there, while cheerfully
disassembling the planet to make a giant computer.
So you recover from your stumble a few thousand
years later inside some secure processor running
on a Matrioshka Brain in their home system,
or maybe around our own Sun, and the aliens
who maintain that simulated Earth keep some
back ups, but you and every person and critter
you know is safely tucked inside a simulated
environment deep inside their territory protected
by the kinds of mega-armadas and defenses
some sprawling K2+ civilizations can muster.
See the Matrioshka Brain and Kardashev Scale
episodes for details.
Maybe when you die you wake up in their afterlife,
or just get stored until the civilization
you are from reaches a level where they’re
comfortable being purely digital and they
pop in to say hi and offer you wider access
to things.
As far as they are concerned, they did you
a favor.
Heck, they didn’t even need to grey goo
your planet, they could have taken mental
snapshots of everyone and left us as is, and
kept those running instead to preserve things.
We might object that they killed us and stuck
us in a simulation, but they might smirk back
at the contradictions in that statement and
regard the objection as being the same as
someone complaining that taking a photograph
of them stole their soul.
That’s what I meant earlier about the Zoo
Hypothesis being both a very bad and very
good solution to the Fermi Paradox, and also
about how it didn’t really matter.
Because it is essentially the Simulation Hypothesis
at that point, and we’ve discussed that
in terms of the Fermi Paradox before also.
Whether the Universe we are in is real or
not, doesn’t make too much difference to
the Fermi Paradox.
First, all our observations about the Universe
that leads us to seeing a Fermi Paradox wouldn’t
apply to the Universe simulating us, the true
reality as it were, which might be some 4
dimensional place, or have stars that actually
orbit planets and trillions of them per galaxy
for all we know, with space being a nice shade
of blue instead of black.
Second, if they are simulating the original
Universe, or just simulating an entire made-up
Universe, it can be assumed they kept it decently
self-consistent, so that it makes sense on
inspection.
Meaning that the Fermi Paradox would have
a logical answer internally consistent with
the observable, if fake, universe.
The simulators, who designed the place, can
be expected to not to leave blatant paradoxes
and contradictions all over it if the goal
is to keep us in the dark… of course it
might not be.
But fundamentally, whether it’s that case
or the more classic non-interference approach,
while you can make arguments both for and
against the ethics of these approaches, there
is something kind of smug and superior about
the approach.
That’s an opinion obviously, and I won’t
deny there’s good arguments for keeping
quiet, but for my part I’d tend to think
the best approach to non-interference in a
culture is like the best approach to keeping
people away from your civilization.
In that case we said you wouldn’t hide,
you’d just do the equivalent of hanging
no trespassing signs around your territory.
For non-interference, I’d tend to think
you’d be best off just introducing yourself,
and telling them how to reach you if they
want to talk.
It seems more practical and ethical, and the
alternatives don’t really seem viable anyhow,
except of course for showing up and uploading
the entire planet, which is undeniably pretty
effective.
Next week we’ll be looking at the other
approach, of directly contacting primitive
civilizations with little technology or even
no technology at all, and tweaking their minds
and physiology to be able to use technology,
a concept called Uplifting.
That will be a two part collaboration episode
with John Michael Godier, with the first part
on this channel and the second on his.
The week after that we will be starting a
discussion on Artificial Intelligence with
a look at Androids, and we will try to sort
out some common myths and misconceptions we
often get on that topic.
The week after that it will be back to the
Outward Bound series to look at Colonizing
Titan, and we will explore the option for
colonizing Saturn’s largest moon along with
looking at some of the concepts for robotic
colonization of the solar system.
For alerts when those and other episodes come
out, be sure to subscribe to the channel.
If you enjoyed this episode, hit the like
button, share it with others.
Until next time, thanks for watching, and
have a great week!
