This episode is brought to you by Brilliant
Humanity’s technological progress in the last  
few centuries has been staggering, lifting 
us up to the heavens above themselves. But  
what if our own ingenuity is bringing us ever 
close to an inevitable doom of our own making?
The Fermi Paradox is perhaps the greatest mystery 
to arise from our greater understanding of Life,  
the Universe, and Everything, as we developed 
telescopes to see distant stars, and microscopes  
to see tiny cells. The sheer scope of, not only 
life on Earth, but the apparent existence of  
uncountable trillions of other worlds that would 
likely match our basic starting conditions,  
makes us pause to wonder how, in such a vast and 
ancient Universe, we could possibly be alone. 
The sheer scope of that Universe is mind-crushing 
as we contemplate how immense our own planet and  
history is, while still being an insignificant 
speck in space and time, a tiny pale blue dot.  
Our reality is both terrifying and awesome, and 
that would surely also describe any civilizations  
which arose long before us, and crafted 
kingdoms among the stars themselves. What  
technologies and artifices they must possess, 
what wonders and horrors could they construct? 
And this is our vantage point today, because, 
for good or ill, we do not seem to see those  
immense empires and their gargantuan capabilities 
in our galaxy. Many reasons are suggested for why,  
and often, it is the notion that we genuinely are 
alone. That, some aspect of our initial creation  
is incredibly improbable, or that the paths 
to intelligence and technology are far harder  
than we assume, and rarely completed. We are 
alone and stand at the pinnacle of our world,  
and will soon claim a galaxy to forge 
to our purposes and aspirations. 
And yet, there is that dreadful fear that we are 
alone; not because the road to technology is hard,  
but rather, because the destination is easy 
to reach and claim, but that technology is not  
simply a knife on which the reckless may cut 
themselves, but a delicious time bomb that we  
arm simply by touching, and an addiction 
that, once tasted, can not be put down,  
even if you see the clock counting down.
That, technology is a ticking time bomb,  
waiting to get us, or a drug we will almost 
inevitably overdose on. That, the silence we hear  
when we aim our ears to the heavens is the silence 
of the grave, the chant of unanswered calls,  
echoing through the emptiness of a tomb, and 
that, we too, will almost certainly join that  
chorus. That, a thousand years from now, the only 
sign we were ever here in our galaxy, will be our  
fading radio signals reaching out to ears that 
cannot hear, as they too, are now long dead. 
Humanity has doubtlessly passed many hurdles to 
get where we are, and we call these ‘filters’  
in regard to the Fermi Paradox, things which, on 
other worlds, many did not pass. Some may be ones  
that many pass, but many do not, and we call those 
the Lesser Filters, and they may be independent  
of a world or clade of species, like an asteroid 
striking a planet, that’s big enough to cause mass  
extinction or even sterilization. Others may be 
stronger filters, ones which few pass, but which  
we would still expect to see countless survivors 
of, given the sheer enormity of planets out there.  
Still, others are what we call Great Filters, 
those challenges where we would expect passage  
no more often than we would someone winning the 
lottery. Collectively, we call the filters we  
have already passed the “Early Filters”.
And, of course, for all that your odds of  
winning a lottery are slim, they are not none, and 
someone does win, indeed thousands every year, so,  
given that there are more stars in this galaxy 
than there are humans on this planet, and more  
galaxies than there are humans too, we usually 
assume those Great Filters must be steep indeed,  
or that the sum total of early filters just 
makes your odds of getting to the end of the  
early filters so slim, that no one has passed them 
before us, who is near enough for us to hear from. 
But, we have the late filters too, because, it 
is not really about if we could hear a twin of  
ourselves on the other side of the galaxy, if 
they happened to exist long enough back in time  
that their signal could have crossed that gulf of 
stars and reached us today. Rather, it is implicit  
in the assumption that they be both willing, 
and able, to spread themselves to the stars,  
not just their radio signals, so that they, or 
their divergent descendants, might still be around  
today, and near us, where we could hear them.
There are two critical late filters under which  
almost every scenario falls. First is that it is 
possible and practical to engage in interstellar  
colonization, and second is that species survive 
long enough to get the knowledge and opportunity  
to spread out. These two combined, have long 
represented one of the largest factions on  
Fermi Paradox solutions, and indeed, essentially 
the very first one. That paradox is named for  
Enrico Fermi, who helped design the first nuclear 
reactor, who was there for the detonation of the  
first nuclear bomb, and who helped design the even 
more immense hydrogen bomb. All of which happened  
before any man-made object, let alone a human, 
left this planet. Indeed, even the first radio  
telescopes were not even a generation old yet, 
and Frank Drake had yet to propose his equation  
predicting the probability of success in the 
search for extraterrestrial intelligence, or SETI. 
As such, it is probably no surprise that Enrico 
Fermi and many others of that era simply assumed  
that interstellar colonization was 
either impossible or impractical,  
while alternatively, blowing ourselves up 
was not. It really was not until the 1970s,  
on the heels of the Apollo missions and many 
successful uncrewed missions to other planets,  
that anyone outside of science fiction really 
seemed to think we could get to the stars one day,  
and Star Trek itself didn’t premier till 22 years 
after Fermi’s death. Even then, there was a common  
assumption in science and sci-fi circles that 
any progress we made to the stars would probably  
be interrupted by a Nuclear War, or several. 
Usually, it was a question of if we would survive,  
and folks often thought that passing this filter 
would require a fundamental change in our nature. 
After the Cold War ended, nuclear war was viewed 
as less likely, and we typically thought of  
it in the context of a rogue state getting 
their hands on a few warheads, devastating,  
but not world-ending, and better modeling of 
fallout and nuclear winter scenarios made it  
seem unlikely that even a full-scale nuclear 
exchange would permanently wreck the planet.  
Nonetheless, nuclear energy was often regarded 
as that potentially ticking time bomb, that,  
if we allowed ourselves to use it, even for 
peaceful ends, we would become addicted to it,  
and so skillful in its use, that it becomes 
prevalent enough that preventing wide-access to  
nuclear bombs would become almost impossible 
and their use to become almost inevitable. 
It may be so too. I may be an optimist but I’m 
also a realist, and it is not currently possible  
for someone to create a nuclear bomb entirely 
in their basement with enough resources, and  
without help from others. But, like every other 
physicist, I know how to make a nuclear reactor  
or nuclear bomb, even if either would be a clumsy 
Model-T equivalent, and I can see where certain  
technologies would make it possible for someone 
with the right knowledge to get the job done.  
3D printers aren’t magic, as we looked at 
in our Santa Claus Machine episode. However,  
it would not take too many improvements to current 
technology such that they should be able to make  
a few of the critical devices for nukes in a 
proverbial basement. As to Uranium itself, you’ve  
got plenty enough in your own backyard, about 
3 parts per million in typical American soil. 
Personally, especially given that I am a big 
fan of nuclear power and breeder reactors,  
I’d just view that as an excuse for civilizations 
to remove background radioactive materials from  
their environment, where they slowly kill us 
anyway, but the key point is, it raises the idea  
that many technologies which are beneficial have a 
dangerous side that only get easier to access, as  
your knowledge and experience with the technology 
grows. Simultaneously, the technology is not just  
growing more dangerous, but growing harder to get 
rid of, as it’s more entrenched in daily life,  
and its removal has very definite results, not 
just theoretical ones. That’s a frequent issue  
in many environmental and ecological efforts. 
We fear what will eventually happen if we don’t  
stop doing something, and may have to weigh 
that against more concrete knowledge of what  
will happen if we just stopped doing some practice 
tomorrow. This doesn’t assume any maliciousness,  
stupidity, or insanity on the part of any of the 
actors involved, which can happen too of course,  
just people faced with hard choices 
and great uncertainties in making them. 
Alternatively, we do have crazy, stupid, and 
malicious people. One loony getting their hands  
on a nuke is not the end of the world, but we have 
a lot of loonies, and the fear of them can be even  
more destabilizing than the activity itself. A 
lot of our tragedies, while frequently killing or  
injuring fewer folks than died of natural causes 
elsewhere during the event, obviously have major  
cultural, economic, and political consequences, 
far out of proportion with their direct damage. 
So, nuclear is that main example of 
a possible technological time bomb,  
but not the first that folks have suggested. The 
internal combustion engine and our reliance on  
fossil fuels is often viewed as one, and we’ve had 
those longer than nukes. Further back, monocrops  
over crop rotation or polyculture, often ruined 
large tracts of arable land and local ecologies,  
and we often worry we’re severely damaging 
our ecology’s robustness, irrespective of  
carbon dioxide concerns by land and sea usage 
practices that are hurting ecological diversity,  
damaging pollinators, depleting nutrients, 
or encouraging rapid migration of invasive  
species without their normal predators 
and parasites that limit their growth.  
Those aren’t really world-ending scenarios, 
probably anyway, but they are examples of how  
technology gives us something we really need or 
want and it’s very hard to decouple it. There’s  
often a very real fear that the sacrifice 
required to do so would be literal human  
sacrifice. For that matter, a technological time 
bomb for the Fermi Paradox need not end the world,  
just prevent galactic colonization from 
occurring even on astronomically long timelines. 
Now, for my part, I don’t think we’d often have 
a technology we knew was innately and certainly  
lethal to our civilization and just turn a 
blind eye to it, but you can obviously make  
a case about a civilization being in denial, and 
it’s certainly a lot easier to see truths if they  
are not unpleasant ones that require a cost. So, 
none of us really have a hard time believing that,  
if a technology existed that, when developed, puts 
us on an inevitable path to ruin, we might take  
a long time coming to believe those raising the 
alarm and possibly might not until it is too late. 
That begs the questions of when is too late, and 
too late for whom, as technology often permits  
some impressive feats of survival and regrowth. 
Indeed, as we’ve looked at in other episodes,  
certain plausible advanced technologies would seem 
to allow disaster recovery that would rival or  
exceed Noah’s Ark, see our episode: Evacuating 
Earth, for some of the extreme examples. 
In any event, you can’t avoid the 
catastrophe if you’re in denial about it,  
but you also have to have some means 
of even knowing about it. You could  
have a hypothetical technology that showed no 
evidence of being harmful until it exploded.  
We might imagine a wormhole to another dimension 
that provided unlimited free energy in the form  
of harmless visible photons coming through. They 
are incredibly easy to manufacture and can only be  
done in relatively low-power-density fashions, 
maybe a few watts each, and so we install them  
into phones and install whole arrays of them 
into houses and cars and such. They’re tricky to  
usefully weaponize beyond just being economic 
and logistical miracles. Unbeknownst to us,  
it’s a property of these awesome dimensions that 
they undergo periodic flashes of higher photon  
energy. Instead of harmless visible 
light coming out the tiny wormhole,  
every couple hundreds years or so, it rapidly 
boosts frequency into the high-gamma range. 
Suddenly, every vehicle, phone, house, pacemaker, 
brain chip, streetlight, and satellite, all become  
powerful gamma-ray emitters. Death is near instant 
for everyone, even folks not living on Earth,  
after all, this power technology sure made 
space colonization easier and it’s running  
every ship and every power plant on deep 
space habitats or interstellar colonies. 
If that sounds like a fun story to read, 
Isaac Asimov’s novel: The Gods Themselves,  
uses something of a similar concept, no spoilers 
beyond that, except that it was the first sci-fi  
novel he’d written after a 15 year hiatus on 
the genre, and his own favorite of his sci-fi  
writings, which were numerous and excellent.
Anyway, this is what I usually call a suicide  
pact technology, or a honey-pot or honey trap 
technology, as a variant of the term used in  
espionage and cybersecurity for seducing 
someone or generally tempting targets.  
Some technology whose dangers cannot be seen in 
foresight and which is just too tempting not to  
use everywhere. That’s critical, because 
if every battery on this planet right now,  
exploded like a grenade, we would still have a lot 
of survivors, even as widespread as batteries are,  
and same, if we were using something like a black 
hole power generator and we turned out to be all  
wrong about how black holes work, so it either 
expired and blew up the whole planet, or ate it.  
The technology itself makes colonization so easy 
that there would be bound to be some colonies and  
they should get forewarning of cataclysms.
Even if they didn’t know what did it,  
as one after another cooked off and the absence 
of further signals was the only forewarning of  
what happened, you’d expect some folks to head 
for the high hills, metaphorically speaking,  
and someone is bound to come to suspect the 
near-infinite power source was a possible cause  
and bunker up somewhere without it. Indeed in a 
big enough civilization prone to paranoia, a trait  
that should not be rare in intelligent species, 
you might expect some folks to blame something  
logically absurd for the cause, like drinking too 
much coffee, or a dubious means to predict it,  
like their horoscope, so if it turned out unknown 
rare planetary conjunctions caused the event,  
some would survive, by hitting on the cause by 
luck. But on the other hand, if every battery  
on this planet did suddenly start cooking off 
like a grenade, even if that was taking place  
over months, it might be a real long time before 
anyone managed to determine that, especially given  
the chaotic impact on civilization implied by 
such a random catastrophe happening everywhere. 
Now, a Suicide Pact Technology, or SPT, is one 
where I tend to assume the civilization does  
know about the dangers of the tech or finds out 
when it is still entirely possible to break usage  
off. I think it’s implied there that they have to 
be in denial about it but we could hypothesize a  
civilization knowing and believing it had that 
impact but thinking they had an escape clause or  
would inevitably get one. Or maybe that the result 
is a good thing, like an ascendance machine that’s  
going to transport all of their minds to a higher 
state of being but would vaporize their planet in  
the process. Given that we have had even large 
groups of folks do that sort of thing before,  
like suicide cults, it isn’t really all that 
implausible to think a very believable design  
might be more persuasive, or that an alien 
species might be more inclined to be persuaded,  
though, that doesn’t pass muster under 
Exclusivity, which is our Fermi Paradox  
argument that asks if a given solution for one 
civilization would plausibly apply to virtually  
all of them. Some would seem likely to be 
more or less credible or gullible than us. 
So, that’s a suicide pact technology or SPT, 
though these are mostly in house terms that we  
often use a bit interchangeably with each other.
For today’s purpose and discussion, we will define  
a Honeypot Technology, or HPT, as one where 
it is innately tempting to use it and there  
are no obvious signs of the technology being 
dangerous until its embedded into civilization.  
This need not be automatically fatal either, 
just any technology that’s got some heavy costs  
on the backend for using that you don’t see until 
it’s embedded. There’s certainly plenty of these,  
though your mileage may vary on what qualifies, 
and in the Fermi Paradox context of course we  
do mean lethal examples for HPTs. Honey Trap 
Technology, or HTTs, we will define those as  
being where someone really does know it’s 
dangerous, but hides or lies about that,  
and these are also common and also somewhat in the 
eye of the beholder, but a lot of addictive drugs  
presumably qualify as this. In a Fermi Paradox 
context this might be where some ancient alien  
race likes to beam out diagrams to wonderful 
new technologies that they know are lethal  
and they know are too tempting not to build.
Though, it could also include something of a  
double trap, like a group of immoral leaders 
finding a great technological way to do  
brainwashing or indoctrination that eventually 
leads to a totally obedient and loyal populace  
and those leaders then being unable to resist 
employing it without realizing that it's going  
to backfire on them too, down the road. In a 
case like that, it’s a lot more believable they  
wouldn’t predict that to backfire, since they’re 
presumably not able to expose that technology to  
a lot of open critical inspection, review 
and discussion. There’s an episode of the  
classic Twilight Zone, “the Obsolete Man”, that 
particularly comes to mind for how brainwashing  
by totalitarian states can get those at the top 
eventually too, and a great classic to catch. 
Now, the problem with beaming out dangerous 
technology is that, while that diagram can move  
at light speed, thus outpacing any colony fleets 
you might send to claim the galaxy, or galaxies,  
it is almost bound to raise some eyebrows, 
or whatever aliens use to express skepticism.  
I’m going to guess that skepticism is a common 
trait among any intelligent and social species,  
and something of a prerequisite for developing 
science and advanced technology too, such as  
radio receivers. If I were the big evil alien 
empire here, I’d probably not say ‘this device for  
cheap power’ because it makes folks wonder where 
everyone else is who previously benefited from it. 
I’d probably say “This is an automated beacon, 
broadcasting for eternity using this awesome  
power source. We, and presumably everyone since 
us, have benefited from this technology’s ability  
to open doorways to uninhabited multiverse 
versions of our own world. Thus, you can  
travel and colonize near-infinite copies of your 
home world without needing to risk century long  
voyages to barren planets and ages of terraforming 
when you can step right into worlds where the only  
difference is however many millions of 
years ago your first primitive ancestor  
made that step up from animal to intelligent 
proto-human – or proto-alien – they instead got  
killed by their curiosity before spreading 
their genes and it hasn’t re-emerged yet.” 
Then you explain how it can also be used 
for cheap power – carefully dialing a small  
gateway to your planet or sun’s hot core 
– and attach a warning that many copies  
of your world might have dangerous 
circumstances on them and here’s an  
algorithm for dialing to planets that are 
safe. And somewhere around number 100 is  
the one that casually dials to the reality 
where that world is a giant black hole. 
It raises the big question of why there’s no other 
signals – of course they might be broadcasting  
plenty of fakes, testimonials from those worlds 
they helped out, which are actually other worlds  
they later colonized broadcasting fake signals. 
The rare handful that figure it out might prefer  
to lie low, or even just figure that anyone 
stupid enough to fall for it had it coming, less  
competition for the galaxy, and that everyone else 
out there is either dead, or in on the hustle,  
and would target and destroy anyone who tried 
to inform on them. Or maybe the big empire just  
floods the radio waves with countless lies content 
to assume the tiny amount of truth slipping in  
from others is just getting buried and ignored 
like the 1 millionth entry on your web browser  
search results. You actually do see a bright and 
vibrant galactic empire out there, it just happens  
to be lies and propaganda. Of course, we do not 
see that, so, unless it turns out that there’s  
some radio alternative, like a Hyperwave or 
Galactic internet that we invariably and easily  
stumble on and it turns out to be full of garbage, 
we can assume this scenario is not going on. 
Maybe instead it’s wormhole gate technology 
to other parallel multiverses but it’s called  
a one way trip, you either use it to go to 
a new world and can’t return, or you use it  
to suck power from other realities and can’t go 
there. All one way. Except, in a case like that,  
you would expect plenty of folks to migrate 
through the mysterious gates and plenty more  
to use the other method, one way power generation. 
I can’t imagine that, if it only allowed one way  
travel out of the Universe, it would ever get 
universal usage, something we discussed more in  
our Aloof Aliens episode. So, you would expect 
your tricks to leave lots of people behind. 
Critical notion though, it is hard to picture 
recipients not being worried about deception  
and looking their gift horses in the mouth. 
If you offer a technology that’s supposed  
to be wildly beneficial, they presumably 
will ask where all the beneficiaries went,  
and any scenario justifying that has to be very 
convincing and, even so, is still likely to raise  
eyebrows… or again, whatever protrusion a given 
alien might raise or move, to indicate skepticism. 
On the other hand, the simple fact that we warn 
people about getting conned and also routinely  
sentence people to jail for running con games, 
doesn’t seem to have resulted in either ceasing  
to happen. People still get conned, or try to 
con others, and an ancient and advanced alien  
race might be really clever. We don’t usually 
think of hyper-intelligent AI or aliens being  
as persuasive as, and more charming than a used 
spaceship sales-being, but that’s probably in  
the wheelhouse of supersmart critters and 
I could believe that a clever adult could  
come up with a line of claptrap that would fool 
99.9% of 4-year-olds, so, we shouldn’t assume  
a hyper-advanced alien civilization doesn’t 
know how to swindle primitives like us with  
99.9% effectiveness. The remaining clever folks 
they sweep up with giant armadas and space guns. 
So, a Honey Trap Tech or HTT is one intentionally 
used to sabotage others, which may or may not also  
get you, and they differ from a Honey Pot Tech, or 
HPT, in that there’s an assumption of deliberate  
malice or deceit in their use. Both would be 
subsets of Technological Timebombs I think,  
and I think we’ve illustrated those examples 
enough, and you can probably come up with others,  
and also come up with reasons why many might not 
work. Feel free to post both for discussion in the  
comments on this episode or on our various social 
media discussion forums, linked in the episodes...  
which is also a great opportunity to hit that 
like or subscribe button while you’re at it. 
One other example to close out on for the day is 
that, we can make a pretty good case that there  
aren’t various technologies which are potential 
suicide pacts, rather, it is technology itself  
that is the time bomb. It is tantalizing and 
addictive and while earlier on it’s a nice option,  
we are essentially totally reliant on it now, 
the same way we are with food or water. Indeed,  
we need those technologies to get food or water, 
practically speaking. Folks often discuss going  
back to a more primitive existence, we even 
looked at that in our Techno-Primitivism and  
Techno-Barbarian episodes, and I think you can 
make a very strong case that, once a civilization  
or species starts down that road to using 
technology, it will likely grow more and be more  
reliant on it, and learn more and more dangerous 
ways to use it. That, inevitably, it will cross  
a threshold where dangerous technologies 
are so easily available that it’s just  
a statistics game until they self-annihilate.
In the absence of any evidence of a threat though,  
I don’t think we can assume the silence in 
the heavens is the silence of the grave. That,  
we are just the next world in a long 
line of victims of our own cleverness.  
To a degree, it becomes a worldview – or galaxy 
view, I suppose – either knowledge is a worthy  
pursuit in spite of its many dangers along the 
road, and will offer paths of salvation from those  
doomsdays we create with it, or it’s a road that 
almost inevitably leads straight to damnation.  
I choose to assume the former.
Ultimately, the only way to know  
if Technology is a Time Bomb, is to wait out 
the clock to see if it explodes, so neither I  
nor anyone else watching this today can give a 
definite answer on that. Of course, if you’re  
watching this thousands of years from now or an 
alien watching this thousands of light years away,  
you presumably know the answer. I always assume a 
large part of our audience might be historians or  
aliens, even if they just get a kick watching 
to see how often wrong I am, but then on this  
particular topic, if I am wrong, they presumably 
aren’t inclined to laugh, or exist to do so.
So we were talking about a lot of dangerous 
technologies that might wipe out humanity,  
everything from nuclear power to artificial 
intelligence to faster than light travel,  
and also how often dangerous technologies can 
cause some irrational worries for folks too. In a  
technological society it's vital for us to be able 
to see dangers where they are, not turn a blind  
eye to them or exaggerate them into boogeymen, 
and the key to that is knowing the science and  
good critical, scientific thinking. These are 
also knowledge and skills that are increasingly  
valuable in nearly every career and everyday life.
But Math and science can be daunting topics to  
many but they don’t need to be, and our friends 
over at Brilliant have courses from beginner  
level to advanced, to suit where you are, 
including wonderful interactive courses on  
both scientific thinking and Everyday Math, to 
help with those key foundational knowledges that  
you can build on. The best learning is hands-on 
and interactive learning, and Brilliant is an  
amazing tool for learning STEM interactively.
Brilliant makes it easier for anyone to learn,  
be it the basics or advanced materials, and 
is the perfect partner for a lifetime learner. 
With Brilliant, you can learn at your own pace, 
learn on the go, and learn something new. To get  
started for free, visit brilliant.org/IsaacArthur 
or click on the link in the description,  
and the first 200 people will get 20% off 
Brilliant's annual premium subscription.
 
So we were talking about possible doomsday 
technologies and one example of that would  
be a technology that literally wipes your 
civilization out backwards in time, and we  
will be exploring that and other dangerous and 
weaponized uses of Time Travel next week, and how  
those function inside of various temporal models 
like alternate timelines. Before that though,  
this weekend is our monthly Scifi Sunday episode, 
Dumbest Alien Invasions, where we’ll examine the  
weirdest attempts and motives in fiction to invade 
Earth. Then in two weeks we’ll ask what humanity’s  
first space settlement will be like, and where 
it will be: in orbit, on the Moon or Mars or  
somewhere else. Then we’ll close the month 
out with our Livestream Q&A on Sunday August  
28th at 4pm Eastern time, where we take your 
questions from the chat and answer them live. 
If you want alerts when those and other episodes 
come out, don’t forget to subscribe to the channel  
and hit the notifications bell. And if you 
enjoyed today’s episode, and would like  
help support future episodes, please visit our 
website, Isaac Arthur.net, for ways to donate,  
or become a show patron over at Patreon. Those 
and other options, like our awesome social media  
forums for discussing futuristic concepts, 
can be found in the links in the description. 
Until next time, thanks for 
watching, and have a great week!
